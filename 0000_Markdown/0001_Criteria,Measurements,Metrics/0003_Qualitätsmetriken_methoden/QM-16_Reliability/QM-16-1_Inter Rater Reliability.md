---
Name: QM-16-1 Interrater-Reliabilität
Title: QM-16-1 Interrater-Reliabilität
TitleGer: QM-16-1 Interrater-Reliabilität
shortdesc: Inter-Rater-Reliabilität misst die Übereinstimmung zwischen verschiedenen Beurteilern oder Bewertern
tags:
  - Qualitätsmetrik
  - "#ML-Reliability"
  - ML-Reliability
ID:
  - QM-16-1
ListMetricID: 
ListMeasureID:
  - "'MA-15'"
  - "'MA-07'"
  - "'MA-06'"
MID: "22"
type:
  - metrik
lcstep: post
CodeEx: true
share: true
---
## QM-16-1 Interrater-Reliabilität

### Beschreibung

Interrater-Reliabilität misst die Übereinstimmung zwischen verschiedenen Beurteilern oder Bewertern. Ein häufig verwendetes Maß ist Cohens Kappa, das die beobachtete Übereinstimmung unter Berücksichtigung der zufälligen Übereinstimmung berechnet.


### Formel

![Interrater-Reliability](../../../../9999_Images/InterRater_Reliability.png)


### Sourcecode "Interrater Reliability"

| RefID | Verweis                                |
| ----- | -------------------------------------- |
| 17    | QM-16-1_Inter Rater Reliability_python |



### Referenzen

| RefID | Verweis                   | Kurzbeschr.                                                                                                                                                                                          |
| ----- | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 123   |  Cohens Kappa             | Cohen's Kappa ist ein statistisches Maß für die Interrater-Reliabilität, das die Übereinstimmung zwischen zwei Beurteilern bewertet. Es kann auch für die Intrarater-Reliabilität verwendet werden.  |
| 241   |  Interrater-Reliabilität  | Die Interrater-Reliabilität misst in der empirischen Sozialforschung die Übereinstimmung von Einschätzungsergebnissen verschiedener Beobachter und dient als Maß für die Objektivität einer Methode. |
